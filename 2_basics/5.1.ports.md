# exposing kubernetes ports

> that's really taking your set of deployments, your pods and
> creating a persistant endpoint that can talk to them

- we can decide to expose them internally in cluster or 
externally to outside sources (means allow them accept connections)

- `kubectl expose`: creats a service for existing pods

> `service` is an endpoint that is consistant and anything inside or
> outside the cluster can access it

- when you create pods in kubernetes, they don't automatically get a
DNS name for external connectivity with IP address.

- if we want to connect to pods, we need a service on top of existing pod.

- `CoreDNS`: allows us to resolve services by name, types of services are:
`ClusterIp`, `NodePort`, `LoadBalancer` and `ExternalName`

### kubernetes service DNS (CoreDNS basics)

- when you are in a container and cURL service name, you get back the 
response so there's a DNS server in there doing sth to match services to
DNS names, believe it or not DNS is optional as a service or an add on
inside your kubernetes cluster.

- internal DNS is provided by CoreDNS means when you create a service, 
you get the hostname that matches the service

### namespaces

> an orginazational parameter or a virtual wall inside the same cluster
> to avoid name clashes between pods, services, deployments inide it.

- in the same namespace, to access services: `curl <hostname>`

- in different namespace, to access services: `curl <hostname>.<namespace>.svc.cluster.local`

## ClusterIp (default)

> it's only available in the cluster (only good in cluster) 
> so it's about one set of kubernetes pods talking to another set of pods and 
> it gets its own DNS stress and that's going to be the DNS address in the 
> core DNS control plane so it's going to get an IP address in virtual IP address 
> space inside the cluster.

``` zsh
# start minikube cluster
minikube start

# start http-server
kubectl create deployment httpenv --image=bretfisher/httpenv

# 
kubectl scale deployment/httpenv --replicas=5

# create the service
kubectl expose deployment/httpenv --port 8888

# generator is the type of template we want the run cmd to use
kubectl run --generator run-pod/v1 tmp-shell --rm -it --image bretfisher/netshoot -- bash

# curl will hits one of the pods and returns its information
# curl [service-ip]:[service-port]
bash-5.0# curl httpenv:8888

# if you are on linux, as you know all nodes and containers by default can talk to
# each other, because your local-machine (node) has access to private ip addresses
# focus that your host don't know the DNS name like what we did inside pod (above-cmd)
# because the name-resolution is inclusive to be inside cluster
curl [service-cluster-IP]:[service-port]
```

- focus that our own `ClusterIP` is inside the cluster for node inside it
to be able access it.

- the service name that we created becomes part of the DNS name for this service

- to summerize we create virtual IP inside the cluster for others to access it with
friendly service name (here httpenv)


## NodePort

> designed for outside of cluster to talk to the service through the
> IP addresses on the nodes themselves, it allocates `high port` to each port and
> the port is open on every node's IP and anyone can connect (if they can reach node)
> and other pods need to be updated to this port

- lets create IP that's exposed externally on host-IP

- by creating `NodePort`, it created a `ClusterIP` because it's how it connect
and it redirect the high port to that `ClusterIP` for that service

``` zsh
# create NodePort service (default type is ClusterIP)
kubectl expose deployment/httpenv --port 8888 --name httpenv-np --type NodePort

# by curling this high port, it hits to one of our pods
curl localhost:32270

# delete service
kubectl delete service/httpenv-np service/httpenv
```

- when get services, you see the port section is sth like: `8888:32270/TCP`
the port on the left is one inside the cluster and container itself that's
listening on and port on right is port on nodes exposed to the outside

- external port range is: [30000,32767]
as you can see it's a high port range, the hope is that there's no conflict

## LoadBalancer 

> mostly used in cloud, it controls a LB endpoint external to the
> cluster and only available when infra provider gives you a LB, then it creates
> `NodePort` and `ClusterIp` services, tells LB to send to `NodePort`

- it isn't built in by default and the only way to use it is through an external
service usually in cloud service and it's a plugin in your kubernetes vendor

``` zsh
# create LoadBalancer service (default type is ClusterIP)
kubectl expose deployment/httpenv --port 8888 --name httpenv-lb --type LoadBalancer

# by curling this high port, it hits to one of our pods
curl localhost:8888

# delete service
kubectl delete service/httpenv-lb deployment/httpenv
```

- when get `LoadBalancer`, you see the port section is sth like: `8888:32250/TCP`
and that's because it will pass localhost:8888 `LoadBalancer` traffic to 
`NodePort` localhost:32250.

## ExternalName
> adds `CNAME DNS` record to `CoreDNS` only and not used for pods
> but for giving pods a DNS name to use for sth outside kubernetes

## kubernetes ingress 
> designed for HTTP traffic


### notes

- `ClusterIp` and `NodePort` (these service) are always available in kubernetes.

- `ClusterIp`< `NodePort`< `LoadBalancer`: means by creating a high level, the
bottoms will be created by that one.